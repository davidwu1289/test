import pandas as pd
import gcsfs

def compare_gcs_dirs(dir1, dir2):
    fs = gcsfs.GCSFileSystem()

    # List the Parquet files in both directories
    files1 = fs.glob(dir1 + '/*.parquet')
    files2 = fs.glob(dir2 + '/*.parquet')

    # Make sure the directories contain the same number of files
    assert len(files1) == len(files2)

    # Compare the data in each pair of corresponding files
    for file1, file2 in zip(files1, files2):
        df1 = pd.read_parquet('gcs://' + file1)
        df2 = pd.read_parquet('gcs://' + file2)

        # Compare the dataframes
        assert df1.equals(df2)

# Use the function to compare two GCS directories
compare_gcs_dirs('gs://test1/folder1', 'gs://test1/folder2')


pip install gcsfs pandas


def handle_bad_data(new_nested_object, path_args, key_name, wrapped_key):
    # Add the logic to handle bad data here
    # For example, save the bad data to a different location

    columnName = "record"
    surrogate_type = columnName.upper()

    print(str(new_nested_object))
    table_item = {"table": {"headers": [{"name": columnName}], "rows": [{"values": [{"string_value": new_nested_object}]}]}}

    element = error_records_dlp(path_args.project_id, table_item, columnName, surrogate_type, key_name, wrapped_key)

    # Prepare the path
    source_time_stamp = get_jedi_file_timestamp()
    file_name = path_args.dec_file_name
    file_name_mod = str(file_name).replace('.txt','')
    srcTimestamp = datetime.datetime.strptime(source_time_stamp, '%Y%m%d%H%M%S%f')
    year = str(srcTimestamp.year)
    month = '{:02d}'.format(srcTimestamp.month)
    day = '{:02d}'.format(srcTimestamp.day)

    blob_name = f'bad_data/year={year}/month={month}/day={day}/{file_name_mod}.log'

    # Create a new blob and upload the dlp_part
    blob = bucket.blob(blob_name)
    blob.upload_from_string(element)

