from google.cloud import storage
import pandas as pd
import pyarrow.parquet as pq
from io import BytesIO

# Initialize a storage client
storage_client = storage.Client()

def compare_gcs_dirs(bucket_name, dir1, dir2):
    # Get the bucket
    bucket = storage_client.get_bucket(bucket_name)

    # List all files in dir1 and dir2
    blobs1 = bucket.list_blobs(prefix=dir1)
    blobs2 = bucket.list_blobs(prefix=dir2)

    # Convert to lists
    files1 = [blob.name for blob in blobs1]
    files2 = [blob.name for blob in blobs2]

    # Make sure the directories contain the same number of files
    assert len(files1) == len(files2), "Number of files in the directories do not match."

    # Compare the data in each pair of corresponding files
    for file1, file2 in zip(sorted(files1), sorted(files2)):
        # Read the files
        blob1 = bucket.blob(file1)
        blob2 = bucket.blob(file2)

        df1 = pd.read_parquet(BytesIO(blob1.download_as_bytes()))
        df2 = pd.read_parquet(BytesIO(blob2.download_as_bytes()))

        # Make sure the dataframes are equal
        pd.testing.assert_frame_equal(df1, df2, check_dtype=False)

    print("Data in both directories is identical.")

# Replace these with your actual bucket name and directory paths
bucket_name = 'my-bucket'
dir1 = 'folder1'
dir2 = 'folder2'

compare_gcs_dirs(bucket_name, dir1, dir2)

